% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Network.R
\name{Network}
\alias{Network}
\title{Neural Net}
\value{
\verb{matrix<numeric>} A matrix with predictions from the network for each datapoint in input_data
}
\description{
The user interface for setting up, training and generating forecasts from a simple neural net.
}
\details{
The training algorithm is speed-up by implementing the matrix form of the backpropagation algorithm by \href{https://doi.org/10.48550/arXiv.2107.09384}{Ostwald and Usee (2021)}.
}
\examples{

## ------------------------------------------------
## Method `Network$new`
## ------------------------------------------------

layer_config <- list(
list(number_of_nodes = 3, activation_fun = "relu", random_init = NULL),
list(number_of_nodes = 1, activation_fun = "sigmoid", random_init = NULL)
)
ANN <- Network$new(cost_fun = "cross_entropy", layer_config = layer_config,
                   input_data = matrix(runif(24), ncol = 6, nrow = 4),
                   output_data = as.matrix(c(1,0,0,1)))

}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-new}{\code{Network$new()}}
\item \href{#method-calculate}{\code{Network$calculate()}}
\item \href{#method-get_all_weights}{\code{Network$get_all_weights()}}
\item \href{#method-set_input_data}{\code{Network$set_input_data()}}
\item \href{#method-set_output_data}{\code{Network$set_output_data()}}
\item \href{#method-train}{\code{Network$train()}}
\item \href{#method-clone}{\code{Network$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-new"></a>}}
\if{latex}{\out{\hypertarget{method-new}{}}}
\subsection{Method \code{new()}}{
Initialize a new class instance.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Network$new(cost_fun = "quadratic", layer_config, input_data, output_data)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{cost_fun}}{\code{character} Declare the cost function to be used. Currently implemented: \code{quadratic} and \code{cross_entropy} (only for classification).}

\item{\code{layer_config}}{\code{list} Defines the parametrization of each desired layer, except for the input_layer (which does not need to be parameterized).
Therefore it is of length \code{L-1} with \code{L} being the number of all layers. Each list itself is a list, containing the input arguments of \link[=Layer]{Layer$new()}. See the example for more details.}

\item{\code{input_data}}{\verb{matrix<numeric>} The input data to be used for classification/regression. Each column corresponds to a different feature and each row corresponds to a different observation.}

\item{\code{output_data}}{\verb{matrix<numeric>} The labels (encoded as numerics) or targets to be used for classification/regression. Each row corresponds to a different observation.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
\code{Network}: The class instance. Invisibly, for chaining.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{layer_config <- list(
list(number_of_nodes = 3, activation_fun = "relu", random_init = NULL),
list(number_of_nodes = 1, activation_fun = "sigmoid", random_init = NULL)
)
ANN <- Network$new(cost_fun = "cross_entropy", layer_config = layer_config,
                   input_data = matrix(runif(24), ncol = 6, nrow = 4),
                   output_data = as.matrix(c(1,0,0,1)))

}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-calculate"></a>}}
\if{latex}{\out{\hypertarget{method-calculate}{}}}
\subsection{Method \code{calculate()}}{
Calculate the output of the network for the complete input_data
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Network$calculate()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-get_all_weights"></a>}}
\if{latex}{\out{\hypertarget{method-get_all_weights}{}}}
\subsection{Method \code{get_all_weights()}}{
Get the weight matrices of all layers.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Network$get_all_weights()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
\verb{list<matrix<numeric>>} A list, containing the weight matrices of all layers. The first element will always
be \code{NULL}, because it corresponds to the input layer.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-set_input_data"></a>}}
\if{latex}{\out{\hypertarget{method-set_input_data}{}}}
\subsection{Method \code{set_input_data()}}{
Set/overwrite the input_data of the layer
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Network$set_input_data(input_data)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{input_data}}{\verb{matrix<numeric>}.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-set_output_data"></a>}}
\if{latex}{\out{\hypertarget{method-set_output_data}{}}}
\subsection{Method \code{set_output_data()}}{
Set/overwrite the output_data of the network
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Network$set_output_data(output_data)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{output_data}}{\verb{matrix<numeric>}}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-train"></a>}}
\if{latex}{\out{\hypertarget{method-train}{}}}
\subsection{Method \code{train()}}{
Train the neural net using backpropagation.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Network$train(batch_size = NULL, epochs = 10000, learning_rate = 0.1)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{batch_size}}{\code{integer} The size of the batch used for updating the weights per epoch.
If \code{batch_size = NULL} or \code{batch_size = n} (\code{n}: sample size), then batch gradient descend will be performed and
the update of the weights will take place AFTER ALL observations are processed (once per epoch).
On the other hand, if \code{batch_size = 1}, then stochastic gradient descend will be performed and the weights will be
updated after each observation (\code{n} times per epoch).
For \verb{1 < batch_size < n} mini batch gradient descend will be performed and the update of the weights will take place
after a set of \code{batch_size} observations is processed.}

\item{\code{epochs}}{\code{integer} The number of epochs used for training.}

\item{\code{learning_rate}}{\code{numeric} defines, how much the weight parameters should be adjusted conditional on the value of the derivative.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
\verb{vector<numeric>} The value of the cost function BEFORE each epoch, so the first element corresponds to the
randomly initiolized model.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-clone"></a>}}
\if{latex}{\out{\hypertarget{method-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Network$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
