% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Layer.R
\name{Layer}
\alias{Layer}
\title{Layer of a neural net}
\description{
The Layer class contains all functionalities needed (e.g. calculate the layer, calculate the derivative
of its weight matrix, etc.) for its use in the \link{Network} class.
One layer is not able to function on its own (aside from the input layer), the class is intentionally not exported.
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-new}{\code{Layer$new()}}
\item \href{#method-forward_pass}{\code{Layer$forward_pass()}}
\item \href{#method-backward_pass}{\code{Layer$backward_pass()}}
\item \href{#method-get_derivatives}{\code{Layer$get_derivatives()}}
\item \href{#method-get_number_of_nodes}{\code{Layer$get_number_of_nodes()}}
\item \href{#method-get_params}{\code{Layer$get_params()}}
\item \href{#method-set_params}{\code{Layer$set_params()}}
\item \href{#method-set_input}{\code{Layer$set_input()}}
\item \href{#method-update_params}{\code{Layer$update_params()}}
\item \href{#method-clone}{\code{Layer$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-new"></a>}}
\if{latex}{\out{\hypertarget{method-new}{}}}
\subsection{Method \code{new()}}{
Initialize a new layer
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Layer$new(
  number_of_nodes,
  activation_fun,
  prior_layer = NULL,
  a_prev = NULL,
  random_init = NULL
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{number_of_nodes}}{\code{integer} that specifies the number of nodes in the layer.}

\item{\code{activation_fun}}{\code{character} that contains the activation function to be used. See details.}

\item{\code{prior_layer}}{\code{Layer} in case of a hidden or output layer, the previous layer.}

\item{\code{a_prev}}{\verb{matrix<numeric>} in case of the layer being the input layer, a\code{matrix} with numeric entries that contains the input_data.}

\item{\code{random_init}}{\verb{list<matrix/vector>}: If \code{NULL} initializesthe weights with random N(0, 0.01) values and the bias with zeros.
If a list with elements \code{weights} and \code{bias} (correct dimensions) is given, uses those values for initialization.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
Denote \code{a}the output of the activation function and \code{z} its input. Currently, the following activation functions are supported:
\itemize{
\item \code{relu}: \eqn{a = max(0,z)}
\item \code{identity}: \eqn{a=z}
\item \code{sigmoid}: \eqn{a = \frac{1}{1+\exp(-z)}}
\item \code{gelu}: \eqn{z \cdot \Phi(z)}
\item \code{silu}: \eqn{a = \frac{z}{1+\exp(-z)}}
\item \code{softplus}: \eqn{ a = \log(1+\exp{z})}
}
}

\subsection{Returns}{
\code{Layer}: The class instance. Invisibly, for chaining.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-forward_pass"></a>}}
\if{latex}{\out{\hypertarget{method-forward_pass}{}}}
\subsection{Method \code{forward_pass()}}{
Calculate the output of the current layer
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Layer$forward_pass()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
\verb{matrix<numeric>} Either the input- (for an input layer) or the output of the activation function.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-backward_pass"></a>}}
\if{latex}{\out{\hypertarget{method-backward_pass}{}}}
\subsection{Method \code{backward_pass()}}{
Calculate the derivative of the current layer wrt to its weights.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Layer$backward_pass(da = 1)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{da}}{\verb{matrix<numeric>}: derivative of the cost function wrt the activations of the next layer..}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
\verb{list<matrix<numeric>>} A list with two elements:
\itemize{
\item \code{derivative}: The derivative for the weighting matrix.
\item \code{w_times_delta}: A matrix product needed for the calculation of the previous layer.
}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-get_derivatives"></a>}}
\if{latex}{\out{\hypertarget{method-get_derivatives}{}}}
\subsection{Method \code{get_derivatives()}}{
Get all cached derivatives.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Layer$get_derivatives()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
\code{list} of derivatives.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-get_number_of_nodes"></a>}}
\if{latex}{\out{\hypertarget{method-get_number_of_nodes}{}}}
\subsection{Method \code{get_number_of_nodes()}}{
Get the number of nodes of the layer.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Layer$get_number_of_nodes()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
\code{integer} The number of nodes.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-get_params"></a>}}
\if{latex}{\out{\hypertarget{method-get_params}{}}}
\subsection{Method \code{get_params()}}{
Get the weight matrix and bias of the layer.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Layer$get_params()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
\verb{matrix<numeric>} The weights of the current layer.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-set_params"></a>}}
\if{latex}{\out{\hypertarget{method-set_params}{}}}
\subsection{Method \code{set_params()}}{
Set/overwrite the weight matrix of the layer.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Layer$set_params(weight_mat, bias_vec = NULL)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{weight_mat}}{\code{matrix} with numeric entries.}

\item{\code{bias_vec}}{\code{vector} with numeric entries.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-set_input"></a>}}
\if{latex}{\out{\hypertarget{method-set_input}{}}}
\subsection{Method \code{set_input()}}{
Set/overwrite the input data of the layer.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Layer$set_input(a_prev)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{a_prev}}{\code{matrix} with numeric entries.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-update_params"></a>}}
\if{latex}{\out{\hypertarget{method-update_params}{}}}
\subsection{Method \code{update_params()}}{
Update weights and biases after backward pass.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Layer$update_params(learning_rate)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{learning_rate}}{\code{numeric}: value between 0 and 1 that governs the sensitivity towards the derivatives.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-clone"></a>}}
\if{latex}{\out{\hypertarget{method-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Layer$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
